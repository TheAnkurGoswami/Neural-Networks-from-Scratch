{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Mechanism Concepts\n",
    "\n",
    "This notebook provides an overview of the attention mechanisms implemented in this module, including their core formulas and operational principles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaled Dot-Product Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a core component allowing the model to weigh the importance of different parts of the input sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Pass\n",
    "The attention is calculated as:\n",
    "$$ \n",
    "\\text{scores} = Q K^T \n",
    "$$ \n",
    "$$ \n",
    "\\text{scaled\\_scores} = \\frac{\\text{scores}}{\\sqrt{d_k}} \n",
    "$$ \n",
    "$$ \n",
    "\\text{attention\\_weights} = \\text{softmax}(\\text{scaled\\_scores}) \n",
    "$$ \n",
    "$$ \n",
    "\\text{output} = \\text{attention\\_weights} V \n",
    "$$ \n",
    "Where:\n",
    "- $Q$, $K$, $V$ are Query, Key, and Value matrices, respectively.\n",
    "- $d_k$ is the dimension of the Key vectors.\n",
    "- `softmax` is the softmax function applied to the scaled scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward Pass\n",
    "Given $dA = \\frac{\\partial L}{\\partial A}$ (the gradient of the loss $L$ with respect to the attention output $A$), the gradients with respect to $Q$, $K$, and $V$ (denoted $dQ_{proj}$, $dK_{proj}$, $dV_{proj}$) are computed. Let $S$ represent `scaled_scores`.\n",
    "\n",
    "1.  Gradient w.r.t. $V$ (Value):\n",
    "    $$ \n",
    "    dV_{\\text{proj}} = \\text{softmax}(S)^T dA \n",
    "    $$ \n",
    "\n",
    "2.  Gradient w.r.t. softmax output ($\\text{softmax}(S)$):\n",
    "    $$ \n",
    "    d\\text{softmax}(S) = dA V^T \n",
    "    $$ \n",
    "\n",
    "3.  Gradient w.r.t. scaled scores ($S$), passing through `softmax.backprop`:\n",
    "    Let $dS' = d\\text{softmax}(S)$.\n",
    "    $$ \n",
    "    dS = \\text{softmax.backprop}(dS') \n",
    "    $$ \n",
    "    (This typically involves $ \\text{softmax}(S) \\odot (dS' - \\text{sum}(dS' \\odot \\text{softmax}(S))) $, where $\\odot$ is element-wise multiplication).\n",
    "\n",
    "4.  Gradient w.r.t. unscaled scores (undo scaling):\n",
    "    $$ \n",
    "    d(QK^T) = \\frac{dS}{\\sqrt{d_k}} \n",
    "    $$ \n",
    "\n",
    "5.  Gradient w.r.t. $Q_{\\text{proj}}$ (Query):\n",
    "    $$ \n",
    "    dQ_{\\text{proj}} = d(QK^T) K \n",
    "    $$ \n",
    "\n",
    "6.  Gradient w.r.t. $K_{\\text{proj}}$ (Key):\n",
    "    $$ \n",
    "    dK_{\\text{proj}} = d(QK^T)^T Q \n",
    "    $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Head Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multi-Head Attention allows the model to jointly attend to information from different representation subspaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Pass\n",
    "1. Input $X$ is linearly projected to create Query ($Q$), Key ($K$), and Value ($V$) matrices for each head $i$: $Q_i, K_i, V_i$.\n",
    "2. Each head computes attention using Scaled Dot-Product Attention:\n",
    "   $$ \n",
    "   \\text{head}_i = \\text{Attention}(Q_i, K_i, V_i) \n",
    "   $$ \n",
    "3. The outputs of all heads are concatenated: $\\text{Concat}(\\text{head}_1, ..., \\text{head}_h)$.\n",
    "4. The concatenated output is linearly projected by an output weight matrix $W^O$:\n",
    "   $$ \n",
    "   \\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, ..., \\text{head}_h) W^O \n",
    "   $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward Pass\n",
    "The backward pass involves:\n",
    "1. Propagating the incoming gradient $dA$ (gradient of loss w.r.t. MultiHead output) through the final output projection $W^O$ to get the gradient w.r.t. the concatenated head outputs.\n",
    "2. Splitting this gradient and propagating it through each attention head ($\\text{head}_i$) to get gradients for $Q_i, K_i, V_i$.\n",
    "3. Concatenating these head-specific gradients to get gradients for the full (pre-split) $Q, K, V$ projections.\n",
    "4. Propagating these gradients through the initial $Q, K, V$ linear projection layers (which are instances of the `Projection` class described below). This updates the weights of these projection layers and computes the gradient w.r.t. the original input $X$. If $Q, K, V$ derive from the same input $X$, their gradients w.r.t. $X$ are summed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Projection Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A specialized `Dense` layer used for creating Query, Key, Value, and output projections in attention mechanisms. It primarily performs a linear transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Pass\n",
    "$$ \n",
    "Z = XW + b \n",
    "$$ \n",
    "Where:\n",
    "- $X$ is the input.\n",
    "- $W$ is the weights matrix.\n",
    "- $b$ is the bias vector.\n",
    "- $Z$ is the linear output.\n",
    "Typically, no activation function is applied directly within these projection layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward Pass\n",
    "Given $dZ = \\frac{\\partial L}{\\partial Z}$ (the gradient of the loss $L$ w.r.t the layer's linear output $Z$):\n",
    "\n",
    "1. Gradient for weights $W$ (denoted $dW$):\n",
    "   Calculated from $X$ and $dZ$. For inputs $X$ of shape (batch, seq_len, in_feat) and $dZ$ of shape (batch, seq_len, out_feat), $dW$ is typically computed by summing $X^T dZ$ contributions across the batch and sequence dimensions appropriately. The implementation details show summation over the batch dimension after an initial batched matrix multiply: $dW_{\\text{batched}} = X^T dZ$, then $dW = \\text{sum}(dW_{\\text{batched}}, \\text{axis}=0)$.\n",
    "   $$ \n",
    "   dW = \\sum_{\\text{batch}} (X^T dZ) \\quad (\\text{conceptual, actual sum might involve sequence too}) \n",
    "   $$ \n",
    "\n",
    "2. Gradient for bias $b$ (denoted $dB$):\n",
    "   Summed over batch and sequence dimensions of $dZ$.\n",
    "   $$ \n",
    "   dB = \\sum_{\\text{batch}} \\sum_{\\text{sequence}} dZ \n",
    "   $$ \n",
    "\n",
    "3. Gradient for input $X$ (denoted $dX$):\n",
    "   $$ \n",
    "   dX = dZ W^T \n",
    "   $$ \n",
    "The weights $W$ and bias $b$ are then updated using these gradients and an optimizer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
