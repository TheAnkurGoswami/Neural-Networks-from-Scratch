{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layers\n\n",
    "This document provides an overview of the neural network layers implemented in this module, including their forward and backward pass formulas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dense (Fully Connected Layer)\n\n",
    "A fully connected neural network layer (Linear layer)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Pass\n",
    "The layer performs a linear transformation of the input data \\(X\\) using a weights matrix \\(W\\) and an optional bias vector \\(b\\), followed by an activation function \\(f\\).\n\n",
    "The operations are:\n",
    "1. Linear transformation:\n",
    "   $$ \n",
    "   Z = XW + b\n",
    "   $$ \n",
    "2. Activation:\n",
    "   $$ \n",
    "   A = f(Z)\n",
    "   $$ \n",
    "Where:\n",
    "- \\( X \\) is the input data.\n",
    "- \\( W \\) is the weights matrix.\n",
    "- \\( b \\) is the bias vector (if `add_bias` is True).\n",
    "- \\( Z \\) is the linear output.\n",
    "- \\( A \\) is the activation output.\n",
    "- \\( f \\) is the activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward Pass\n",
    "Calculates gradients with respect to inputs (\\( dX \\)), weights (\\( dW \\)), and bias (\\( dB \\)), and updates weights and bias.\n\n",
    "The steps are:\n",
    "1. Compute gradient with respect to the output of the linear part (before activation):\n",
    "   $$ \n",
    "   dZ = dA \\odot f'(Z)\n",
    "   $$ \n",
    "   Where \\( dA = \\frac{\\partial L}{\\partial A} \\) is the gradient of the loss with respect to the layer's activation output, and \\( f'(Z) = \\frac{\\partial A}{\\partial Z} \\) is the derivative of the activation function. Thus, \\( dZ = \\frac{\\partial L}{\\partial Z} \\).\n\n",
    "2. Compute gradient for weights:\n",
    "   $$ \n",
    "   dW = X^T dZ\n",
    "   $$ \n\n",
    "3. Compute gradient for bias (if bias is used):\n",
    "   $$ \n",
    "   dB = \\sum_{\\text{batch}} dZ\n",
    "   $$ \n",
    "   (sum over the batch dimension).\n\n",
    "4. Compute gradient with respect to the input of this layer:\n",
    "   $$ \n",
    "   dX = dZ W^T\n",
    "   $$ \n\n",
    "The weights \\(W\\) and bias \\(b\\) are then updated using these gradients and an optimizer."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
