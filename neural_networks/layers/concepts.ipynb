{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layer Concepts\n",
    "\n",
    "This notebook provides an overview of the neural network layers implemented in this module, including their forward and backward pass formulas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dense (Fully Connected Layer)\n",
    "\n",
    "A fully connected neural network layer, also known as a Linear layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Pass\n",
    "The layer performs a linear transformation of the input data $X$ using a weights matrix $W$ and an optional bias vector $b$, followed by an activation function $f$.\n",
    "\n",
    "The operations are:\n",
    "1. Linear transformation: $Z = XW + b$\n",
    "   $$ \n",
    "   Z = XW + b \n",
    "   $$ \n",
    "2. Activation: $A = f(Z)$\n",
    "   $$ \n",
    "   A = f(Z) \n",
    "   $$ \n",
    "Where:\n",
    "- $X$ is the input data.\n",
    "- $W$ is the weights matrix.\n",
    "- $b$ is the bias vector (if `add_bias` is True).\n",
    "- $Z$ is the linear output (input to the activation function).\n",
    "- $A$ is the activation output.\n",
    "- $f$ is the activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward Pass\n",
    "Calculates gradients with respect to inputs ($dX$), weights ($dW$), and bias ($dB$), and then these gradients are used by an optimizer to update the weights and bias.\n",
    "\n",
    "The steps are:\n",
    "1. Compute gradient with respect to the output of the linear part (before activation), $dZ$:\n",
    "   $$ \n",
    "   dZ = dA \\odot f'(Z) \n",
    "   $$ \n",
    "   Where $dA = \\frac{\\partial L}{\\partial A}$ is the gradient of the loss $L$ with respect to the layer's activation output $A$, and $f'(Z) = \\frac{\\partial A}{\\partial Z}$ is the derivative of the activation function. Thus, $dZ = \\frac{\\partial L}{\\partial Z}$.\n",
    "\n",
    "2. Compute gradient for weights, $dW$:\n",
    "   $$ \n",
    "   dW = X^T dZ \n",
    "   $$ \n",
    "\n",
    "3. Compute gradient for bias, $dB$ (if bias is used):\n",
    "   $$ \n",
    "   dB = \\sum_{\\text{batch}} dZ \n",
    "   $$ \n",
    "   (This sum is over the batch dimension).\n",
    "\n",
    "4. Compute gradient with respect to the input of this layer, $dX$:\n",
    "   $$ \n",
    "   dX = dZ W^T \n",
    "   $$ \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
