{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Functions\n\n",
    "This document provides an overview of the loss functions implemented in this module, including their forward and backward pass formulas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CrossEntropyLoss\n\n",
    "Computes the cross-entropy loss between predicted probabilities and true labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Pass\n",
    "For a single sample, the cross-entropy loss is:\n",
    "$$ \n",
    "L_i = - \\sum_{c=1}^{C} y_{true_{ic}} \\log(y_{pred_{ic}}) \n",
    "$$ \n",
    "where \\( C \\) is the number of classes.\n\n",
    "The total loss is the average over all \\( N \\) samples in the batch:\n",
    "$$ \n",
    "L_{CE} = - \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{c=1}^{C} y_{true_{ic}} \\log(y_{pred_{ic}})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward Pass\n",
    "The gradient of the loss with respect to the predicted values (\\(y_{pred}\\)) is computed as:\n",
    "$$ \n",
    "\\frac{\\partial L}{\\partial y_{pred}} = - \\frac{y_{true}}{y_{pred}} \\cdot \\frac{1}{N}\n",
    "$$ \n",
    "*(Note: The factor \\( \\frac{1}{N} \\) where N is batch size is applied in the implementation, derived from the derivative of the averaged forward loss.)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MSELoss (Mean Squared Error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Pass\n",
    "The MSE is calculated as:\n",
    "$$ \n",
    "L_{MSE} = \\frac{1}{N} \\sum_{i=1}^{N} (y_{pred_i} - y_{true_i})^2\n",
    "$$ \n",
    "where:\n",
    "- \\(N\\) is the total number of elements in \\(y_{true}\\).\n",
    "- \\(y_{pred_i}\\) is the i-th predicted value.\n",
    "- \\(y_{true_i}\\) is the i-th true value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward Pass\n",
    "The gradient \\( \\frac{\\partial L_{MSE}}{\\partial y_{pred}} \\) is computed as:\n",
    "$$ \n",
    "\\frac{\\partial L_{MSE}}{\\partial y_{pred}} = \\frac{2}{N} (y_{pred} - y_{true})\n",
    "$$ \n",
    "where:\n",
    "- \\(N\\) is the total number of elements in \\(y_{true}\\)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMSELoss (Root Mean Squared Error)\n\n",
    "RMSE is the square root of MSE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Pass\n",
    "The RMSE is calculated as the square root of the Mean Squared Error (MSE):\n",
    "$$ \n",
    "L_{RMSE} = \\sqrt{L_{MSE}} = \\sqrt{\\frac{1}{N} \\sum_{i=1}^{N} (y_{pred_i} - y_{true_i})^2}\n",
    "$$ \n",
    "where:\n",
    "- \\(N\\) is the total number of elements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward Pass\n",
    "The gradient is derived using the chain rule:\n",
    "$$ \n",
    "\\frac{\\partial L_{RMSE}}{\\partial y_{pred}} = \\frac{\\partial L_{RMSE}}{\\partial L_{MSE}} \\times \\frac{\\partial L_{MSE}}{\\partial y_{pred}}\n",
    "$$ \n\n",
    "Since \\( \\frac{\\partial L_{RMSE}}{\\partial L_{MSE}} = \\frac{1}{2 \\sqrt{L_{MSE}}} = \\frac{1}{2 L_{RMSE}} \\),\n",
    "the gradient becomes:\n",
    "$$ \n",
    "\\frac{\\partial L_{RMSE}}{\\partial y_{pred}} = \\frac{1}{2 L_{RMSE}} \\frac{\\partial L_{MSE}}{\\partial y_{pred}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMSELossV2 (Root Mean Squared Error - Direct Version)\n\n",
    "This version calculates RMSE and its gradient directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Pass\n",
    "The RMSE is calculated directly as:\n",
    "$$ \n",
    "L_{RMSE} = \\sqrt{\\frac{1}{N} \\sum_{i=1}^{N} (y_{pred_i} - y_{true_i})^2}\n",
    "$$ \n",
    "where:\n",
    "- \\(N\\) is the total number of elements in \\(y_{true}\\)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward Pass\n",
    "The gradient \\( \\frac{\\partial L_{RMSE}}{\\partial y_{pred}} \\) is:\n",
    "$$ \n",
    "\\frac{\\partial L_{RMSE}}{\\partial y_{pred}} = \\frac{1}{N \\cdot L_{RMSE}} (y_{pred} - y_{true})\n",
    "$$ \n",
    "where:\n",
    "- \\(N\\) is the total number of elements.\n",
    "- \\(L_{RMSE}\\) is the computed RMSE loss."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
