{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Functions Concepts\n",
    "\n",
    "This notebook provides an overview of the loss functions implemented in this module, including their forward and backward pass formulas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CrossEntropyLoss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computes the cross-entropy loss between predicted probabilities $y_{pred}$ and true labels $y_{true}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Pass\n",
    "For a single sample $i$, the cross-entropy loss $L_i$ is:\n",
    "$$ \n",
    "L_i = - \\sum_{c=1}^{C} y_{true_{ic}} \\log(y_{pred_{ic}}) \n",
    "$$ \n",
    "where $C$ is the number of classes.\n",
    "\n",
    "The total loss $L_{CE}$ is the average over all $N$ samples in the batch:\n",
    "$$ \n",
    "L_{CE} = - \\frac{1}{N} \\sum_{i=1}^{N} \\sum_{c=1}^{C} y_{true_{ic}} \\log(y_{pred_{ic}}) \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward Pass\n",
    "The gradient of the loss $L$ with respect to the predicted values $y_{pred}$ (denoted $\\frac{\\partial L}{\\partial y_{pred}}$) is computed as:\n",
    "$$ \n",
    "\\frac{\\partial L}{\\partial y_{pred}} = - \\frac{y_{true}}{y_{pred}} \\cdot \\frac{1}{N} \n",
    "$$ \n",
    "*(Note: The factor $\\frac{1}{N}$, where N is the batch size, is applied in the implementation, derived from the derivative of the averaged forward loss.)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MSELoss (Mean Squared Error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Pass\n",
    "The Mean Squared Error $L_{MSE}$ is calculated as:\n",
    "$$ \n",
    "L_{MSE} = \\frac{1}{N} \\sum_{i=1}^{N} (y_{pred_i} - y_{true_i})^2 \n",
    "$$ \n",
    "where:\n",
    "- $N$ is the total number of elements in $y_{true}$.\n",
    "- $y_{pred_i}$ is the i-th predicted value.\n",
    "- $y_{true_i}$ is the i-th true value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward Pass\n",
    "The gradient $\\frac{\\partial L_{MSE}}{\\partial y_{pred}}$ is computed as:\n",
    "$$ \n",
    "\\frac{\\partial L_{MSE}}{\\partial y_{pred}} = \\frac{2}{N} (y_{pred} - y_{true}) \n",
    "$$ \n",
    "where $N$ is the total number of elements in $y_{true}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMSELoss (Root Mean Squared Error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RMSE is the square root of MSE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Pass\n",
    "The Root Mean Squared Error $L_{RMSE}$ is calculated as the square root of the Mean Squared Error $L_{MSE}$:\n",
    "$$ \n",
    "L_{RMSE} = \\sqrt{L_{MSE}} = \\sqrt{\\frac{1}{N} \\sum_{i=1}^{N} (y_{pred_i} - y_{true_i})^2} \n",
    "$$ \n",
    "where $N$ is the total number of elements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward Pass\n",
    "The gradient is derived using the chain rule: $\\frac{\\partial L_{RMSE}}{\\partial y_{pred}} = \\frac{\\partial L_{RMSE}}{\\partial L_{MSE}} \\times \\frac{\\partial L_{MSE}}{\\partial y_{pred}}$.\n",
    "\n",
    "Since $\\frac{\\partial L_{RMSE}}{\\partial L_{MSE}} = \\frac{1}{2 \\sqrt{L_{MSE}}} = \\frac{1}{2 L_{RMSE}}$, the gradient becomes:\n",
    "$$ \n",
    "\\frac{\\partial L_{RMSE}}{\\partial y_{pred}} = \\frac{1}{2 L_{RMSE}} \\frac{\\partial L_{MSE}}{\\partial y_{pred}} \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMSELossV2 (Root Mean Squared Error - Direct Version)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This version calculates RMSE and its gradient directly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Pass\n",
    "The RMSE $L_{RMSE}$ is calculated directly as:\n",
    "$$ \n",
    "L_{RMSE} = \\sqrt{\\frac{1}{N} \\sum_{i=1}^{N} (y_{pred_i} - y_{true_i})^2} \n",
    "$$ \n",
    "where $N$ is the total number of elements in $y_{true}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward Pass\n",
    "The gradient $\\frac{\\partial L_{RMSE}}{\\partial y_{pred}}$ is:\n",
    "$$ \n",
    "\\frac{\\partial L_{RMSE}}{\\partial y_{pred}} = \\frac{1}{N \\cdot L_{RMSE}} (y_{pred} - y_{true}) \n",
    "$$ \n",
    "where:\n",
    "- $N$ is the total number of elements.\n",
    "- $L_{RMSE}$ is the computed RMSE loss from the forward pass."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
