{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizers\n\n",
    "This document provides an overview of the optimization algorithms implemented in this module, detailing their update rules. The parameter \\( \\theta \\) is updated as \\( \\theta_t = \\theta_{t-1} - \\text{update_value} \\)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam (Adaptive Moment Estimation)\n\n",
    "Adam computes adaptive learning rates for each parameter.\n\n",
    "Let \\( g_t \\) be the derivative (gradient) at timestep \\( t \\), \\( \\eta \\) be the learning rate, \\( \\beta_1, \\beta_2 \\) be decay rates, \\( m_t \\) be the first moment vector (mean), and \\( v_t \\) be the second moment vector (uncentered variance). The timestep for bias correction is denoted by \\( t \\) (effectively `self._epoch` in the code for Adam)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The update rules are:\n",
    "1. Update biased first moment estimate:\n",
    "   $$ \n",
    "   m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) g_t\n",
    "   $$ \n",
    "2. Update biased second raw moment estimate:\n",
    "   $$ \n",
    "   v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) g_t^2\n",
    "   $$ \n",
    "3. Compute bias-corrected first moment estimate:\n",
    "   $$ \n",
    "   \\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t}\n",
    "   $$ \n",
    "4. Compute bias-corrected second raw moment estimate:\n",
    "   $$ \n",
    "   \\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t}\n",
    "   $$ \n",
    "5. Compute parameter update value:\n",
    "   $$ \n",
    "   \\text{update_value} = \\eta \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon}\n",
    "   $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMSProp\n\n",
    "RMSProp divides the learning rate for a weight by a running average of the magnitudes of recent gradients for that weight.\n\n",
    "Let \\( g_t \\) be the derivative (gradient) at timestep \\( t \\), \\( \\eta \\) be the learning rate, \\( \\rho \\) be the discounting factor, and \\( E[g^2]_t \\) be the moving average of squared gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The update rule is:\n",
    "1. Update moving average of squared gradients:\n",
    "   $$ \n",
    "   E[g^2]_t = \\rho E[g^2]_{t-1} + (1 - \\rho) g_t^2\n",
    "   $$ \n",
    "2. Compute parameter update value:\n",
    "   $$ \n",
    "   \\text{update_value} = \\frac{\\eta}{\\sqrt{E[g^2]_t} + \\epsilon} g_t\n",
    "   $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD (Stochastic Gradient Descent)\n\n",
    "Optionally includes momentum.\n\n",
    "Let \\( g_t \\) be the derivative (gradient) at timestep \\( t \\), \\( \\eta \\) be the learning rate, and \\( \\beta \\) be the momentum factor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If momentum (\\( \\beta > 0 \\)):\n",
    "1. Update velocity:\n",
    "   $$ \n",
    "   v_t = \\beta v_{t-1} + g_t\n",
    "   $$ \n",
    "2. Compute parameter update value:\n",
    "   $$ \n",
    "   \\text{update_value} = \\eta v_t\n",
    "   $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If no momentum (\\( \\beta = 0 \\)):\n",
    "1. Compute parameter update value:\n",
    "   $$ \n",
    "   \\text{update_value} = \\eta g_t\n",
    "   $$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
